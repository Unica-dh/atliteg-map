# Backend API con Upload CSV - Analisi e Progettazione

## Documento di Progettazione Dettagliata
**Data**: 2026-01-10  
**Versione**: 1.0  
**Soluzione**: Backend API con Autenticazione + Upload CSV  
**Riferimento**: [data-security-analysis.md](./data-security-analysis.md) - Soluzione 1

---

## 1. Executive Summary

Questo documento descrive la progettazione dettagliata della **Soluzione 1: Backend API con Autenticazione**, estesa con la funzionalit√† di **upload CSV** e generazione automatica dei file JSON utilizzati dall'applicazione.

### Requisiti Chiave

1. ‚úÖ **Protezione completa dei dati**: Nessun file (CSV o JSON) accessibile direttamente dall'esterno
2. ‚úÖ **Upload CSV sicuro**: Solo utenti autenticati possono caricare file CSV
3. ‚úÖ **Generazione automatica JSON**: Il backend processa il CSV e genera il JSON ottimizzato
4. ‚úÖ **API protette**: Autenticazione JWT/API key per tutte le operazioni
5. ‚úÖ **Rate limiting**: Prevenzione scraping e abusi
6. ‚úÖ **Audit trail**: Log completo di upload e accessi

### Flusso Dati Proposto

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      FLUSSO DATI CON BACKEND API                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. UPLOAD CSV (Admin)
   Admin ‚Üí POST /api/admin/upload (CSV + Auth) ‚Üí Backend
          ‚Üì
   Backend processa CSV ‚Üí Genera JSON ‚Üí Salva in directory privata
          ‚Üì
   Notifica success ‚Üí Admin

2. CONSUMO DATI (Client)
   Browser ‚Üí GET /api/lemmi (Auth) ‚Üí Backend
          ‚Üì
   Backend legge JSON privato ‚Üí Ritorna dati
          ‚Üì
   Browser riceve JSON (no download diretto)

3. PROTEZIONE
   Browser ‚Üí GET /data/lemmi.json ‚Üí Nginx ‚Üí ‚ùå 403 Forbidden
   Browser ‚Üí GET /uploads/*.csv   ‚Üí Nginx ‚Üí ‚ùå 403 Forbidden
```

**Garanzia Sicurezza**: 
- ‚ùå CSV NON scaricabile (directory privata, no endpoint pubblico)
- ‚ùå JSON NON scaricabile direttamente (servito solo via API con auth)
- ‚úÖ Dati accessibili SOLO tramite API autenticata
- ‚úÖ Upload SOLO da admin autenticati

---

## 2. Architettura Componente Backend

### 2.1 Struttura Directory

```
lemmario-dashboard/
‚îú‚îÄ‚îÄ server/                           # Backend Node.js/Express
‚îÇ   ‚îú‚îÄ‚îÄ index.js                      # Entry point server
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.js                 # Configurazione (env vars)
‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data.js                   # GET /api/lemmi, /api/geojson, /api/regions
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ admin.js                  # POST /api/admin/upload, GET /api/admin/status
‚îÇ   ‚îú‚îÄ‚îÄ middleware/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth.js                   # Autenticazione JWT/API key
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adminAuth.js              # Autenticazione admin (ruolo elevato)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rateLimit.js              # Rate limiting
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ upload.js                 # Multer upload middleware
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ errorHandler.js           # Error handling centralizzato
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ csvProcessor.js           # Processamento CSV ‚Üí JSON
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataValidator.js          # Validazione dati CSV
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ auditLogger.js            # Logging operazioni
‚îÇ   ‚îú‚îÄ‚îÄ data/                         # üîí DIRECTORY PRIVATA
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lemmi.json                # Generato da CSV
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ geojson.json              # Generato da CSV
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ limits_IT_regions.geojson # Statico
‚îÇ   ‚îú‚îÄ‚îÄ uploads/                      # üîí CSV caricati (temporanei)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îÇ   ‚îú‚îÄ‚îÄ logs/                         # Audit logs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ audit.log
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ public/                           # SOLO asset pubblici (no data)
‚îÇ   ‚îú‚îÄ‚îÄ logo/
‚îÇ   ‚îî‚îÄ‚îÄ images/
‚îî‚îÄ‚îÄ src/                              # Frontend Next.js
    ‚îî‚îÄ‚îÄ services/
        ‚îî‚îÄ‚îÄ dataLoader.ts             # Modificato per chiamare API
```

### 2.2 Stack Tecnologico Backend

| Componente | Tecnologia | Versione | Scopo |
|------------|------------|----------|-------|
| **Runtime** | Node.js | 20 LTS | Esecuzione JavaScript server-side |
| **Framework** | Express.js | ^4.18 | Web server e routing |
| **Autenticazione** | jsonwebtoken | ^9.0 | JWT token generation/validation |
| **Upload** | multer | ^1.4 | Gestione upload file multipart |
| **CSV Parsing** | papaparse | ^5.5 | Parsing CSV (stesso del frontend) |
| **Rate Limiting** | express-rate-limit | ^7.0 | Prevenzione abusi API |
| **CORS** | cors | ^2.8 | Cross-Origin Resource Sharing |
| **Validazione** | joi | ^17.0 | Schema validation |
| **Logging** | winston | ^3.11 | Logging strutturato |
| **Process Manager** | pm2 | ^5.3 | Gestione processi produzione |

---

## 3. Funzionalit√† Utente

### 3.1 Ruoli Utente

#### Ruolo: **Public User** (Frontend Application)

**Permessi**:
- ‚úÖ Lettura dati tramite API autenticata (con API key frontend)
- ‚ùå Upload CSV
- ‚ùå Modifica dati
- ‚ùå Accesso diretto a file

**Autenticazione**: API key embedded nel frontend (rotazione periodica)

#### Ruolo: **Admin** (Data Manager)

**Permessi**:
- ‚úÖ Upload CSV
- ‚úÖ Visualizzazione stato processamento
- ‚úÖ Download logs (opzionale)
- ‚úÖ Gestione API keys (opzionale)

**Autenticazione**: JWT token con credenziali username/password

### 3.2 User Stories

#### US-1: Upload CSV come Admin

**Come** amministratore dei dati  
**Voglio** caricare un nuovo file CSV con i dati aggiornati  
**Cos√¨ che** l'applicazione utilizzi i dati pi√π recenti senza deployment manuale  

**Acceptance Criteria**:
1. ‚úÖ Login con credenziali admin ‚Üí ricevo JWT token
2. ‚úÖ Upload CSV via form/API ‚Üí backend valida formato
3. ‚úÖ Backend processa CSV ‚Üí genera JSON ottimizzato
4. ‚úÖ Ricevo conferma processamento con statistiche (record processati, errori)
5. ‚úÖ Applicazione frontend utilizza immediatamente nuovi dati (o dopo cache invalidation)
6. ‚úÖ CSV originale salvato in directory privata (backup/audit)

**Flow**:
```
1. Admin ‚Üí POST /api/admin/login
   Body: { username: "admin", password: "***" }
   ‚Üê Response: { token: "eyJhbG..." }

2. Admin ‚Üí POST /api/admin/upload
   Headers: { Authorization: "Bearer eyJhbG..." }
   Body: FormData { file: lemmi.csv }
   ‚Üê Response: { 
       status: "processing",
       jobId: "uuid-123"
     }

3. Admin ‚Üí GET /api/admin/status/:jobId
   ‚Üê Response: {
       status: "completed",
       records: 15234,
       errors: [],
       timestamp: "2026-01-10T15:30:00Z"
     }

4. Frontend app ‚Üí GET /api/lemmi
   ‚Üê Response: { /* nuovi dati JSON */ }
```

#### US-2: Consultazione Dati come Frontend Application

**Come** applicazione frontend  
**Voglio** accedere ai dati tramite API  
**Cos√¨ che** possa visualizzare lemmi, aree geografiche e timeline  

**Acceptance Criteria**:
1. ‚úÖ Chiamo API con API key valida ‚Üí ricevo dati JSON
2. ‚úÖ API restituisce solo dati necessari (no metadati sensibili)
3. ‚úÖ Rate limiting previene download massivo
4. ‚úÖ Nessun accesso diretto a file CSV o JSON sul filesystem

**Flow**:
```
Frontend ‚Üí GET /api/lemmi
Headers: { X-API-Key: "frontend-key-abc123" }
‚Üê Response: [
  {
    IdLemma: 1,
    Lemma: "abbacchio",
    Forma: "abbacchio",
    CollGeografica: "Roma",
    ...
  },
  ...
]
```

#### US-3: Audit e Monitoring come Admin

**Come** amministratore  
**Voglio** visualizzare log di upload e accessi  
**Cos√¨ che** possa monitorare l'utilizzo e identificare anomalie  

**Acceptance Criteria**:
1. ‚úÖ Visualizzazione log upload (chi, quando, risultato)
2. ‚úÖ Statistiche accessi API (rate, IP sorgente)
3. ‚úÖ Alert su upload falliti o accessi anomali

---

## 4. Protezione File CSV e JSON

### 4.1 Protezioni Implementate

#### Livello 1: Filesystem Permissions

```bash
# Directory server/data/ e server/uploads/
chmod 700 server/data server/uploads
chown node:node server/data server/uploads

# File CSV e JSON
chmod 600 server/data/*.json
chmod 600 server/uploads/*.csv
```

**Risultato**: Solo processo Node.js pu√≤ leggere/scrivere file

#### Livello 2: Nginx Configuration

```nginx
server {
    listen 9000;
    root /usr/share/nginx/html;

    # ‚ùå BLOCCO TOTALE directory /data
    location /data/ {
        deny all;
        return 403;
    }

    # ‚ùå BLOCCO directory /uploads
    location /uploads/ {
        deny all;
        return 403;
    }

    # ‚ùå BLOCCO file .csv ovunque
    location ~ \.csv$ {
        deny all;
        return 403;
    }

    # ‚úÖ PROXY API requests a backend Node.js
    location /api/ {
        proxy_pass http://backend:3001;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        # Rate limiting
        limit_req zone=api_limit burst=20 nodelay;
        limit_req_status 429;
    }

    # SPA routing
    location / {
        try_files $uri $uri/ /index.html;
    }
}

# Rate limit zone
limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;
```

**Risultato**: 
- ‚úÖ Richieste a `/data/lemmi.json` ‚Üí HTTP 403
- ‚úÖ Richieste a `/uploads/file.csv` ‚Üí HTTP 403
- ‚úÖ Richieste a `/api/lemmi` ‚Üí Proxy a backend (con auth)

#### Livello 3: Autenticazione Backend

**Per API Dati (Frontend)**: API Key

```javascript
// middleware/auth.js
const authMiddleware = (req, res, next) => {
  const apiKey = req.headers['x-api-key'];
  const validKeys = process.env.FRONTEND_API_KEYS?.split(',') || [];
  
  if (!validKeys.includes(apiKey)) {
    return res.status(401).json({ 
      error: 'Unauthorized',
      message: 'Invalid or missing API key' 
    });
  }
  
  // Logging accesso
  logger.info('API access', {
    endpoint: req.path,
    ip: req.ip,
    apiKey: apiKey.substring(0, 8) + '...'
  });
  
  next();
};
```

**Per Upload Admin**: JWT Token

```javascript
// middleware/adminAuth.js
const jwt = require('jsonwebtoken');

const adminAuthMiddleware = (req, res, next) => {
  const authHeader = req.headers['authorization'];
  const token = authHeader && authHeader.split(' ')[1]; // Bearer <token>
  
  if (!token) {
    return res.status(401).json({ error: 'Missing authentication token' });
  }
  
  try {
    const payload = jwt.verify(token, process.env.JWT_SECRET);
    
    // Verifica ruolo admin
    if (payload.role !== 'admin') {
      return res.status(403).json({ error: 'Insufficient permissions' });
    }
    
    req.user = payload;
    next();
  } catch (error) {
    return res.status(401).json({ error: 'Invalid token' });
  }
};
```

#### Livello 4: Rate Limiting

```javascript
// middleware/rateLimit.js
const rateLimit = require('express-rate-limit');

// Rate limit per API dati (frontend)
const dataApiLimiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15 minuti
  max: 100, // 100 richieste per finestra
  message: 'Too many requests from this IP, please try again later.',
  standardHeaders: true,
  legacyHeaders: false,
});

// Rate limit per upload admin (pi√π restrittivo)
const uploadLimiter = rateLimit({
  windowMs: 60 * 60 * 1000, // 1 ora
  max: 5, // Massimo 5 upload per ora
  message: 'Upload limit exceeded. Please wait before uploading again.',
  skipSuccessfulRequests: false,
});

module.exports = { dataApiLimiter, uploadLimiter };
```

#### Livello 5: HTTPS Only (Produzione)

```nginx
# Redirect HTTP ‚Üí HTTPS
server {
    listen 80;
    server_name atliteg-map.example.com;
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name atliteg-map.example.com;
    
    ssl_certificate /etc/letsencrypt/live/atliteg-map.example.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/atliteg-map.example.com/privkey.pem;
    
    # ... resto configurazione
}
```

### 4.2 Matrice Protezione

| Asset | Accesso Diretto | Via API Pubblica | Via API Admin | Protezione |
|-------|----------------|------------------|---------------|------------|
| **CSV originale** | ‚ùå No | ‚ùå No | ‚ùå No | Filesystem + Nginx |
| **JSON generato** | ‚ùå No | ‚ùå No | ‚ùå No | Filesystem + Nginx |
| **Dati via API** | N/A | ‚úÖ S√¨ (auth) | ‚úÖ S√¨ (auth) | API Key + Rate Limit |
| **Upload CSV** | ‚ùå No | ‚ùå No | ‚úÖ S√¨ (auth) | JWT + Upload Limit |
| **Logs** | ‚ùå No | ‚ùå No | ‚úÖ S√¨ (auth) | JWT + Role Check |

### 4.3 Scenari di Attacco e Mitigazioni

#### Scenario 1: Tentativo Download Diretto JSON

**Attacco**: `GET https://atliteg-map.com/data/lemmi.json`

**Mitigazione**:
1. Nginx blocca request ‚Üí HTTP 403
2. Log tentativo accesso (monitoring)
3. File fisicamente non accessibile da web root

**Risultato**: ‚ùå Attacco fallito

#### Scenario 2: Scraping Massivo via API

**Attacco**: Script automatico chiama `GET /api/lemmi` ripetutamente

**Mitigazione**:
1. Rate limiting: max 100 req/15min per IP
2. Dopo 100 richieste ‚Üí HTTP 429 Too Many Requests
3. Possibile ban temporaneo IP (opzionale)
4. Monitoring anomalie (spike richieste)

**Risultato**: ‚ö†Ô∏è Parzialmente mitigato (pu√≤ ottenere dati ma lentamente)

#### Scenario 3: API Key Leak

**Attacco**: API key frontend esposta in codice sorgente

**Mitigazione**:
1. API key rotazione periodica (es. mensile)
2. Rate limiting per key
3. Monitoring utilizzo per key
4. Possibilit√† disabilitare key compromessa
5. **Nota**: Protezione completa richiede auth utente (OAuth, etc.)

**Risultato**: ‚ö†Ô∏è Rischio residuo accettabile per applicazione pubblica

#### Scenario 4: Upload CSV Malicious

**Attacco**: Admin account compromesso, upload CSV con dati malevoli

**Mitigazione**:
1. Validazione rigorosa CSV (schema, tipi dati, dimensione)
2. Sanitizzazione input (rimozione caratteri speciali)
3. Quarantena file: validazione prima di processare
4. Rollback automatico su errori
5. Audit completo: chi ha caricato cosa e quando
6. Backup automatico file precedente

**Risultato**: ‚úÖ Attacco mitigato con validazione e audit

---

## 5. Implementazione Dettagliata

### 5.1 API Endpoints

#### Endpoint: POST /api/admin/login

**Scopo**: Autenticazione admin e generazione JWT token

**Request**:
```http
POST /api/admin/login
Content-Type: application/json

{
  "username": "admin",
  "password": "secure_password_here"
}
```

**Response Success**:
```json
{
  "success": true,
  "token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "expiresIn": "24h",
  "user": {
    "username": "admin",
    "role": "admin"
  }
}
```

**Response Error**:
```json
{
  "success": false,
  "error": "Invalid credentials"
}
```

**Implementazione**:
```javascript
// routes/admin.js
const express = require('express');
const jwt = require('jsonwebtoken');
const bcrypt = require('bcrypt');
const router = express.Router();

router.post('/login', async (req, res) => {
  try {
    const { username, password } = req.body;
    
    // Validazione input
    if (!username || !password) {
      return res.status(400).json({ 
        success: false, 
        error: 'Username and password required' 
      });
    }
    
    // Verifica credenziali (esempio con env vars)
    const adminUsername = process.env.ADMIN_USERNAME;
    const adminPasswordHash = process.env.ADMIN_PASSWORD_HASH;
    
    if (username !== adminUsername) {
      return res.status(401).json({ 
        success: false, 
        error: 'Invalid credentials' 
      });
    }
    
    const validPassword = await bcrypt.compare(password, adminPasswordHash);
    if (!validPassword) {
      return res.status(401).json({ 
        success: false, 
        error: 'Invalid credentials' 
      });
    }
    
    // Genera JWT
    const token = jwt.sign(
      { 
        username, 
        role: 'admin',
        iat: Math.floor(Date.now() / 1000)
      },
      process.env.JWT_SECRET,
      { expiresIn: '24h' }
    );
    
    // Audit log
    logger.info('Admin login successful', { username, ip: req.ip });
    
    res.json({
      success: true,
      token,
      expiresIn: '24h',
      user: { username, role: 'admin' }
    });
    
  } catch (error) {
    logger.error('Login error', { error: error.message });
    res.status(500).json({ 
      success: false, 
      error: 'Internal server error' 
    });
  }
});

module.exports = router;
```

#### Endpoint: POST /api/admin/upload

**Scopo**: Upload file CSV e trigger processamento

**Request**:
```http
POST /api/admin/upload
Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
Content-Type: multipart/form-data

------WebKitFormBoundary
Content-Disposition: form-data; name="file"; filename="lemmi_updated.csv"
Content-Type: text/csv

IdLemma,Lemma,Forma,Coll.Geografica,...
1,abbacchio,abbacchio,Roma,...
...
```

**Response Success (Async Processing)**:
```json
{
  "success": true,
  "message": "Upload successful, processing started",
  "jobId": "550e8400-e29b-41d4-a716-446655440000",
  "statusUrl": "/api/admin/status/550e8400-e29b-41d4-a716-446655440000"
}
```

**Response Error**:
```json
{
  "success": false,
  "error": "Invalid file format",
  "details": "Expected CSV, got application/octet-stream"
}
```

**Implementazione**:
```javascript
// routes/admin.js
const multer = require('multer');
const { v4: uuidv4 } = require('uuid');
const path = require('path');
const csvProcessor = require('../services/csvProcessor');
const adminAuthMiddleware = require('../middleware/adminAuth');
const { uploadLimiter } = require('../middleware/rateLimit');

// Configurazione upload
const storage = multer.diskStorage({
  destination: (req, file, cb) => {
    cb(null, path.join(__dirname, '../uploads'));
  },
  filename: (req, file, cb) => {
    const timestamp = Date.now();
    const safeFilename = file.originalname.replace(/[^a-zA-Z0-9._-]/g, '_');
    cb(null, `${timestamp}_${safeFilename}`);
  }
});

const upload = multer({
  storage,
  limits: {
    fileSize: 10 * 1024 * 1024, // 10 MB max
    files: 1
  },
  fileFilter: (req, file, cb) => {
    // Accetta solo CSV
    if (file.mimetype === 'text/csv' || 
        file.mimetype === 'application/csv' ||
        file.originalname.endsWith('.csv')) {
      cb(null, true);
    } else {
      cb(new Error('Only CSV files are allowed'));
    }
  }
});

router.post('/upload', 
  adminAuthMiddleware,
  uploadLimiter,
  upload.single('file'),
  async (req, res) => {
    try {
      if (!req.file) {
        return res.status(400).json({
          success: false,
          error: 'No file uploaded'
        });
      }
      
      const jobId = uuidv4();
      const uploadInfo = {
        jobId,
        filename: req.file.filename,
        originalName: req.file.originalname,
        size: req.file.size,
        uploadedBy: req.user.username,
        uploadedAt: new Date().toISOString(),
        status: 'processing'
      };
      
      // Audit log
      logger.info('CSV upload received', uploadInfo);
      
      // Avvia processamento asincrono
      csvProcessor.processCSV(req.file.path, jobId)
        .then(result => {
          logger.info('CSV processing completed', { 
            jobId, 
            records: result.recordCount 
          });
        })
        .catch(error => {
          logger.error('CSV processing failed', { 
            jobId, 
            error: error.message 
          });
        });
      
      res.json({
        success: true,
        message: 'Upload successful, processing started',
        jobId,
        statusUrl: `/api/admin/status/${jobId}`
      });
      
    } catch (error) {
      logger.error('Upload error', { error: error.message });
      res.status(500).json({
        success: false,
        error: error.message || 'Upload failed'
      });
    }
  }
);
```

#### Endpoint: GET /api/admin/status/:jobId

**Scopo**: Verifica stato processamento CSV

**Request**:
```http
GET /api/admin/status/550e8400-e29b-41d4-a716-446655440000
Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
```

**Response**:
```json
{
  "success": true,
  "jobId": "550e8400-e29b-41d4-a716-446655440000",
  "status": "completed",
  "progress": {
    "recordsProcessed": 15234,
    "recordsTotal": 15234,
    "errors": [],
    "warnings": [
      "Line 1023: Missing IdAmbito, skipped"
    ]
  },
  "timing": {
    "startedAt": "2026-01-10T15:30:00Z",
    "completedAt": "2026-01-10T15:30:45Z",
    "duration": "45s"
  },
  "output": {
    "lemmiJson": "/data/lemmi.json",
    "geojsonJson": "/data/geojson.json",
    "recordCount": 15234
  }
}
```

#### Endpoint: GET /api/lemmi

**Scopo**: Recupero dati lemmi (frontend)

**Request**:
```http
GET /api/lemmi
X-API-Key: frontend_key_abc123xyz
```

**Response**:
```json
[
  {
    "IdLemma": 1,
    "Lemma": "abbacchio",
    "Forma": "abbacchio",
    "CollGeografica": "Roma",
    "Latitudine": "41.9028",
    "Longitudine": "12.4964",
    "Anno": "1250",
    "Periodo": "Sec. XIII",
    "Categoria": "Carne",
    "Frequenza": 45,
    "URL": "https://vosl.it/...",
    "IdAmbito": "5"
  },
  ...
]
```

**Rate Limiting**: 100 req/15min per IP

**Implementazione**:
```javascript
// routes/data.js
const express = require('express');
const fs = require('fs').promises;
const path = require('path');
const authMiddleware = require('../middleware/auth');
const { dataApiLimiter } = require('../middleware/rateLimit');

const router = express.Router();

router.get('/lemmi', 
  authMiddleware,
  dataApiLimiter,
  async (req, res) => {
    try {
      const dataPath = path.join(__dirname, '../data/lemmi.json');
      const data = await fs.readFile(dataPath, 'utf-8');
      
      // Parse e ritorna
      const lemmi = JSON.parse(data);
      
      // Log accesso (opzionale, commentabile per performance)
      // logger.debug('Lemmi data accessed', { ip: req.ip });
      
      res.json(lemmi);
      
    } catch (error) {
      logger.error('Error loading lemmi data', { error: error.message });
      res.status(500).json({ 
        error: 'Failed to load data',
        message: 'Internal server error'
      });
    }
  }
);

module.exports = router;
```

### 5.2 Servizio Processamento CSV

**File**: `server/services/csvProcessor.js`

```javascript
const Papa = require('papaparse');
const fs = require('fs').promises;
const path = require('path');
const logger = require('./logger');

class CSVProcessor {
  constructor() {
    this.jobs = new Map(); // In-memory job tracking (pu√≤ essere Redis)
  }
  
  async processCSV(csvFilePath, jobId) {
    const startTime = Date.now();
    
    try {
      // Update job status
      this.jobs.set(jobId, {
        status: 'processing',
        startedAt: new Date().toISOString()
      });
      
      // Leggi CSV
      const csvContent = await fs.readFile(csvFilePath, 'utf-8');
      
      // Parse CSV (same logic as preprocessing script)
      const result = await new Promise((resolve, reject) => {
        Papa.parse(csvContent, {
          header: true,
          skipEmptyLines: true,
          transformHeader: (header) => {
            const headerMap = {
              'IdLemma': 'IdLemma',
              'Lemma': 'Lemma',
              'Forma': 'Forma',
              'Coll.Geografica': 'CollGeografica',
              'Latitudine': 'Latitudine',
              'Longitudine': 'Longitudine',
              'Tipo coll.Geografica': 'TipoCollGeografica',
              'Anno': 'Anno',
              'Periodo': 'Periodo',
              'IDPeriodo': 'IDPeriodo',
              'Datazione': 'Datazione',
              'Categoria': 'Categoria',
              'Frequenza': 'Frequenza',
              'URL': 'URL',
              'IdAmbito': 'IdAmbito',
              'reg_istat_code': 'RegionIstatCode',
            };
            return headerMap[header] || header;
          },
          complete: (results) => resolve(results),
          error: (error) => reject(error)
        });
      });
      
      const lemmi = result.data;
      const warnings = [];
      
      // Validazione base
      if (lemmi.length === 0) {
        throw new Error('CSV file is empty');
      }
      
      // Salva JSON ottimizzato
      const outputPath = path.join(__dirname, '../data/lemmi.json');
      await fs.writeFile(outputPath, JSON.stringify(lemmi, null, 0));
      
      // Backup CSV originale (opzionale)
      const backupPath = path.join(
        __dirname, 
        '../uploads/backup',
        `${jobId}_${path.basename(csvFilePath)}`
      );
      await fs.mkdir(path.dirname(backupPath), { recursive: true });
      await fs.copyFile(csvFilePath, backupPath);
      
      // Rimuovi CSV temporaneo
      await fs.unlink(csvFilePath);
      
      const endTime = Date.now();
      const duration = Math.round((endTime - startTime) / 1000);
      
      // Update job status
      const jobResult = {
        status: 'completed',
        startedAt: this.jobs.get(jobId).startedAt,
        completedAt: new Date().toISOString(),
        duration: `${duration}s`,
        recordCount: lemmi.length,
        warnings,
        output: {
          lemmiJson: outputPath,
          recordCount: lemmi.length
        }
      };
      
      this.jobs.set(jobId, jobResult);
      
      logger.info('CSV processing completed', { jobId, ...jobResult });
      
      return jobResult;
      
    } catch (error) {
      // Update job status
      this.jobs.set(jobId, {
        status: 'failed',
        error: error.message,
        startedAt: this.jobs.get(jobId)?.startedAt,
        failedAt: new Date().toISOString()
      });
      
      logger.error('CSV processing failed', { jobId, error: error.message });
      throw error;
    }
  }
  
  getJobStatus(jobId) {
    return this.jobs.get(jobId) || { status: 'not_found' };
  }
}

module.exports = new CSVProcessor();
```

### 5.3 Configurazione Docker

**File**: `lemmario-dashboard/Dockerfile.backend`

```dockerfile
# Multi-stage build per Backend + Frontend
FROM node:20-alpine AS backend-builder

WORKDIR /app/server

# Install backend dependencies
COPY server/package*.json ./
RUN npm ci --production

# Copy backend code
COPY server/ ./

# ---

FROM node:20-alpine AS frontend-builder

WORKDIR /app

# Install frontend dependencies
COPY lemmario-dashboard/package*.json ./
RUN npm ci

# Copy frontend code and build
COPY lemmario-dashboard/ ./
RUN npm run build

# ---

FROM node:20-alpine AS production

WORKDIR /app

# Install pm2 globally
RUN npm install -g pm2

# Copy backend
COPY --from=backend-builder /app/server ./server

# Copy frontend static build
COPY --from=frontend-builder /app/out ./public

# Create data directories
RUN mkdir -p /app/server/data /app/server/uploads /app/server/logs && \
    chown -R node:node /app

# Copy initial data files (se esistono)
COPY data/*.json /app/server/data/ || true
COPY data/*.geojson /app/server/data/ || true

USER node

EXPOSE 3001

# Start backend with pm2
CMD ["pm2-runtime", "start", "server/index.js", "--name", "atliteg-api"]
```

**File**: `docker-compose.yml` (updated)

```yaml
version: '3.8'

services:
  backend:
    build:
      context: .
      dockerfile: lemmario-dashboard/Dockerfile.backend
    container_name: atliteg-backend
    ports:
      - "3001:3001"
    environment:
      - NODE_ENV=production
      - PORT=3001
      - FRONTEND_API_KEYS=${FRONTEND_API_KEYS}
      - ADMIN_USERNAME=${ADMIN_USERNAME}
      - ADMIN_PASSWORD_HASH=${ADMIN_PASSWORD_HASH}
      - JWT_SECRET=${JWT_SECRET}
      - ALLOWED_ORIGINS=https://atliteg-map.example.com
    volumes:
      - ./server/data:/app/server/data
      - ./server/uploads:/app/server/uploads
      - ./server/logs:/app/server/logs
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    container_name: atliteg-nginx
    ports:
      - "9000:9000"
    volumes:
      - ./lemmario-dashboard/out:/usr/share/nginx/html:ro
      - ./lemmario-dashboard/nginx.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      - backend
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
```

**File**: `.env.example`

```bash
# Backend API Configuration
FRONTEND_API_KEYS=frontend_key_abc123xyz,frontend_key_backup_456

# Admin Credentials
ADMIN_USERNAME=admin
# Generate hash: echo -n "your_password" | bcrypt
ADMIN_PASSWORD_HASH=$2b$10$abcdefghijklmnopqrstuv...

# JWT Secret (generate with: openssl rand -hex 32)
JWT_SECRET=your_jwt_secret_here_min_32_chars

# CORS
ALLOWED_ORIGINS=https://atliteg-map.example.com,http://localhost:3000
```

---

## 6. Workflow Completo

### 6.1 Setup Iniziale

```bash
# 1. Clone repository
git clone https://github.com/Unica-dh/atliteg-map.git
cd atliteg-map

# 2. Crea file .env
cp .env.example .env
nano .env  # Configura credenziali

# 3. Genera password hash
npm install -g bcrypt-cli
echo -n "my_admin_password" | bcrypt

# 4. Build e avvia
docker-compose up --build -d

# 5. Verifica salute servizi
docker-compose ps
curl http://localhost:3001/health  # Backend
curl http://localhost:9000/health  # Nginx
```

### 6.2 Upload CSV (Prima Volta)

```bash
# 1. Login admin
curl -X POST http://localhost:3001/api/admin/login \
  -H "Content-Type: application/json" \
  -d '{
    "username": "admin",
    "password": "my_admin_password"
  }' \
  | jq -r '.token' > token.txt

TOKEN=$(cat token.txt)

# 2. Upload CSV
curl -X POST http://localhost:3001/api/admin/upload \
  -H "Authorization: Bearer $TOKEN" \
  -F "file=@data/Lemmi_forme_atliteg_updated.csv" \
  | jq '.'

# Output:
# {
#   "success": true,
#   "jobId": "550e8400-e29b-41d4-a716-446655440000",
#   "statusUrl": "/api/admin/status/550e8400-e29b-41d4-a716-446655440000"
# }

# 3. Verifica stato
JOB_ID="550e8400-e29b-41d4-a716-446655440000"
curl -X GET "http://localhost:3001/api/admin/status/$JOB_ID" \
  -H "Authorization: Bearer $TOKEN" \
  | jq '.'
```

### 6.3 Consumo Dati Frontend

```typescript
// lemmario-dashboard/src/services/dataLoader.ts
const API_BASE_URL = process.env.NEXT_PUBLIC_API_URL || 'http://localhost:3001';
const API_KEY = process.env.NEXT_PUBLIC_API_KEY;

export async function loadCSVData(): Promise<Lemma[]> {
  try {
    const response = await fetch(`${API_BASE_URL}/api/lemmi`, {
      headers: {
        'X-API-Key': API_KEY || ''
      }
    });
    
    if (!response.ok) {
      throw new Error(`API error: ${response.status}`);
    }
    
    const data = await response.json();
    console.log(`‚úÖ Dati caricati: ${data.length} record`);
    return data;
    
  } catch (error) {
    console.error('‚ùå Errore caricamento dati:', error);
    throw error;
  }
}
```

---

## 7. Testing e Validazione

### 7.1 Test Sicurezza

```bash
# Test 1: Verifica blocco accesso diretto CSV
curl -I http://localhost:9000/data/Lemmi_forme_atliteg.csv
# Atteso: HTTP/1.1 403 Forbidden

# Test 2: Verifica blocco accesso diretto JSON
curl -I http://localhost:9000/data/lemmi.json
# Atteso: HTTP/1.1 403 Forbidden

# Test 3: Verifica API senza autenticazione
curl http://localhost:3001/api/lemmi
# Atteso: HTTP/1.1 401 Unauthorized

# Test 4: Verifica API con chiave invalida
curl -H "X-API-Key: invalid_key" http://localhost:3001/api/lemmi
# Atteso: HTTP/1.1 401 Unauthorized

# Test 5: Verifica API con chiave valida
curl -H "X-API-Key: frontend_key_abc123xyz" http://localhost:3001/api/lemmi
# Atteso: HTTP/1.1 200 OK + JSON data

# Test 6: Verifica rate limiting
for i in {1..120}; do
  curl -H "X-API-Key: frontend_key_abc123xyz" \
       http://localhost:3001/api/lemmi > /dev/null 2>&1
done
# Dopo 100 richieste: HTTP/1.1 429 Too Many Requests

# Test 7: Upload senza autenticazione
curl -X POST http://localhost:3001/api/admin/upload \
  -F "file=@test.csv"
# Atteso: HTTP/1.1 401 Unauthorized

# Test 8: Upload con token scaduto
curl -X POST http://localhost:3001/api/admin/upload \
  -H "Authorization: Bearer expired_token" \
  -F "file=@test.csv"
# Atteso: HTTP/1.1 401 Unauthorized
```

### 7.2 Test Funzionali

```bash
# Test 9: Upload CSV valido
TOKEN="valid_jwt_token_here"
curl -X POST http://localhost:3001/api/admin/upload \
  -H "Authorization: Bearer $TOKEN" \
  -F "file=@data/Lemmi_forme_atliteg_updated.csv" \
  | jq '.'
# Atteso: { "success": true, "jobId": "..." }

# Test 10: Verifica generazione JSON
docker exec atliteg-backend ls -lh /app/server/data/
# Atteso: lemmi.json aggiornato con timestamp recente

# Test 11: Verifica contenuto JSON
docker exec atliteg-backend head -n 20 /app/server/data/lemmi.json
# Atteso: Array JSON con record lemmi
```

---

## 8. Monitoraggio e Manutenzione

### 8.1 Logging

**Winston Logger Configuration**:

```javascript
// server/services/logger.js
const winston = require('winston');
const path = require('path');

const logger = winston.createLogger({
  level: process.env.LOG_LEVEL || 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.json()
  ),
  transports: [
    // Console log
    new winston.transports.Console({
      format: winston.format.combine(
        winston.format.colorize(),
        winston.format.simple()
      )
    }),
    // File log - errori
    new winston.transports.File({
      filename: path.join(__dirname, '../logs/error.log'),
      level: 'error',
      maxsize: 10485760, // 10MB
      maxFiles: 5
    }),
    // File log - audit
    new winston.transports.File({
      filename: path.join(__dirname, '../logs/audit.log'),
      level: 'info',
      maxsize: 10485760,
      maxFiles: 10
    })
  ]
});

module.exports = logger;
```

**Audit Log Format**:

```json
{
  "timestamp": "2026-01-10T15:30:00.123Z",
  "level": "info",
  "message": "CSV upload received",
  "jobId": "550e8400-e29b-41d4-a716-446655440000",
  "filename": "1736524200_lemmi_updated.csv",
  "originalName": "Lemmi_forme_atliteg_updated.csv",
  "size": 1180659,
  "uploadedBy": "admin",
  "ip": "192.168.1.100"
}
```

### 8.2 Health Checks

**Endpoint**: `GET /health`

```javascript
// server/index.js
app.get('/health', (req, res) => {
  const healthcheck = {
    status: 'ok',
    timestamp: new Date().toISOString(),
    uptime: process.uptime(),
    checks: {
      dataFiles: 'checking...'
    }
  };
  
  // Verifica esistenza file dati
  const dataPath = path.join(__dirname, 'data/lemmi.json');
  fs.access(dataPath, fs.constants.R_OK)
    .then(() => {
      healthcheck.checks.dataFiles = 'ok';
      res.status(200).json(healthcheck);
    })
    .catch(() => {
      healthcheck.checks.dataFiles = 'error';
      healthcheck.status = 'degraded';
      res.status(503).json(healthcheck);
    });
});
```

### 8.3 Backup Strategy

```bash
#!/bin/bash
# backup-data.sh

BACKUP_DIR="/backups/atliteg-data"
DATE=$(date +%Y%m%d_%H%M%S)

# Crea directory backup
mkdir -p "$BACKUP_DIR"

# Backup file JSON
docker cp atliteg-backend:/app/server/data/lemmi.json \
  "$BACKUP_DIR/lemmi_$DATE.json"

docker cp atliteg-backend:/app/server/data/geojson.json \
  "$BACKUP_DIR/geojson_$DATE.json"

# Backup CSV uploads
docker cp atliteg-backend:/app/server/uploads/backup \
  "$BACKUP_DIR/csv_$DATE"

# Backup logs
docker cp atliteg-backend:/app/server/logs \
  "$BACKUP_DIR/logs_$DATE"

# Comprimi
tar -czf "$BACKUP_DIR/atliteg_backup_$DATE.tar.gz" \
  "$BACKUP_DIR/*_$DATE.*"

# Rimuovi file non compressi
rm -rf "$BACKUP_DIR/*_$DATE.json" \
       "$BACKUP_DIR/csv_$DATE" \
       "$BACKUP_DIR/logs_$DATE"

# Mantieni solo ultimi 30 giorni
find "$BACKUP_DIR" -name "*.tar.gz" -mtime +30 -delete

echo "‚úÖ Backup completato: atliteg_backup_$DATE.tar.gz"
```

**Cron job** (esegui backup giornaliero):

```bash
# crontab -e
0 2 * * * /opt/atliteg-map/backup-data.sh >> /var/log/atliteg-backup.log 2>&1
```

---

## 9. Roadmap Implementazione

### Fase 1: Backend Base (3 giorni)

**Giorno 1**: Setup Express Server
- [x] Inizializzare progetto Node.js in `server/`
- [x] Installare dipendenze (express, cors, multer, etc.)
- [x] Creare struttura directory
- [x] Configurare logging (winston)
- [x] Implementare health check endpoint
- [x] Test locale server

**Giorno 2**: Autenticazione e Upload
- [x] Implementare JWT authentication
- [x] Creare endpoint `/api/admin/login`
- [x] Configurare multer per upload
- [x] Implementare endpoint `/api/admin/upload`
- [x] Creare servizio `csvProcessor.js`
- [x] Test upload end-to-end

**Giorno 3**: API Dati e Rate Limiting
- [x] Implementare endpoint `/api/lemmi`
- [x] Implementare endpoint `/api/geojson`
- [x] Implementare endpoint `/api/regions`
- [x] Configurare rate limiting
- [x] Implementare API key authentication
- [x] Test API con Postman/curl

### Fase 2: Integrazione Frontend (1 giorno)

**Giorno 4**: Modifica Frontend
- [x] Modificare `services/dataLoader.ts`
- [x] Modificare `hooks/useRegions.ts`
- [x] Aggiungere variabili ambiente (`.env.local`)
- [x] Configurare CORS nel backend
- [x] Test integrazione locale
- [x] Verificare funzionalit√† dashboard

### Fase 3: Docker e Deploy (1 giorno)

**Giorno 5**: Containerizzazione
- [x] Creare `Dockerfile.backend`
- [x] Aggiornare `docker-compose.yml`
- [x] Aggiornare `nginx.conf` (proxy API)
- [x] Creare `.env.example`
- [x] Build Docker images
- [x] Test deploy locale
- [x] Documentazione deployment

### Fase 4: Testing e Hardening (1 giorno - opzionale)

**Giorno 6**: Security & Testing
- [ ] Penetration testing (OWASP ZAP)
- [ ] Load testing (Apache Bench)
- [ ] Verifica HTTPS configuration
- [ ] Implementare backup automatico
- [ ] Setup monitoring (Prometheus/Grafana opzionale)
- [ ] Documentazione operativa

---

## 10. FAQ e Troubleshooting

### Q1: Come ruoto le API keys del frontend?

**A**: 
```bash
# 1. Genera nuova key
NEW_KEY=$(openssl rand -hex 16)
echo "New API Key: $NEW_KEY"

# 2. Aggiungi alle variabili ambiente (mantieni vecchia)
FRONTEND_API_KEYS=frontend_key_abc123xyz,${NEW_KEY}

# 3. Riavvia backend
docker-compose restart backend

# 4. Aggiorna frontend con nuova key
# .env.local
NEXT_PUBLIC_API_KEY=${NEW_KEY}

# 5. Rebuild frontend
npm run build

# 6. Dopo conferma funzionamento, rimuovi vecchia key
FRONTEND_API_KEYS=${NEW_KEY}
docker-compose restart backend
```

### Q2: Cosa succede se l'upload CSV fallisce a met√†?

**A**: 
- File temporaneo rimane in `/uploads` (manuale cleanup o cron job)
- JSON esistente NON viene sovrascritto
- Job status rimane "failed" con dettagli errore
- Admin pu√≤ ri-tentare upload
- Implementare rollback automatico in `csvProcessor.js`

### Q3: Come gestisco dataset molto grandi (>100MB)?

**A**:
- Aumentare `maxFileSize` in multer config
- Implementare streaming parse (no caricamento intero in RAM)
- Considerare chunked upload (multipart)
- Aggiungere progress reporting (WebSocket/Server-Sent Events)
- Possibile timeout Nginx ‚Üí aumentare `proxy_read_timeout`

### Q4: Un utente pu√≤ scaricare comunque i dati facendo molte richieste API?

**A**: 
S√¨, tecnicamente possibile ma:
- Rate limiting rallenta (100 req/15min)
- IP ban possibile dopo soglia
- Monitoring rileva pattern anomali
- **Protezione completa richiederebbe**: OAuth user login, quotas per-utente, CAPTCHA

**Per applicazione pubblica/accademica**: Rischio accettabile, dati eventualmente condivisibili.

### Q5: Come implemento rollback a versione precedente dati?

**A**:
```bash
# 1. Lista backup disponibili
ls -lh /backups/atliteg-data/

# 2. Estrai backup
tar -xzf /backups/atliteg-data/atliteg_backup_20260109_020000.tar.gz

# 3. Copia file nel container
docker cp lemmi_20260109_020000.json atliteg-backend:/app/server/data/lemmi.json

# 4. Verifica
curl -H "X-API-Key: frontend_key_abc123xyz" http://localhost:3001/api/lemmi | jq '.[0]'

# 5. Riavvia backend (opzionale, per invalidare cache)
docker-compose restart backend
```

---

## 11. Conclusioni e Next Steps

### Deliverable Finale

1. ‚úÖ **Backend API completo** con autenticazione e upload
2. ‚úÖ **Frontend integrato** che consuma API protette
3. ‚úÖ **Docker deployment** production-ready
4. ‚úÖ **Documentazione completa** per sviluppatori e operatori
5. ‚úÖ **Script di backup** e manutenzione
6. ‚úÖ **Test suite** per validazione sicurezza

### Protezioni Implementate

| Livello | Protezione | Implementato |
|---------|-----------|--------------|
| **Filesystem** | Permissions 600/700 | ‚úÖ |
| **Nginx** | Blocco `/data/`, `/uploads/` | ‚úÖ |
| **API** | JWT + API Key | ‚úÖ |
| **Rate Limit** | 100 req/15min | ‚úÖ |
| **Upload** | Admin-only, 5 upload/ora | ‚úÖ |
| **Audit** | Winston logging | ‚úÖ |
| **HTTPS** | SSL/TLS (produzione) | ‚ö†Ô∏è Configurare |

### Metriche Successo

- ‚è±Ô∏è **Tempo implementazione**: 5 giorni (stimato)
- üí∞ **Costo operativo aggiuntivo**: +20-30% risorse server
- üîí **CSV accessibili dall'esterno**: ‚ùå NO
- üîí **JSON accessibili direttamente**: ‚ùå NO
- ‚úÖ **Dati accessibili via API**: Solo con autenticazione
- ‚úÖ **Upload CSV**: Solo admin autenticati
- üìä **Audit completo**: S√¨ (chi, cosa, quando)

### Prossimi Step Raccomandati

1. **Implementare backup automatico** con retention policy
2. **Monitoring avanzato** con Prometheus + Grafana (opzionale)
3. **OAuth2 login** per admin (invece di username/password)
4. **Multi-tenancy** se necessario gestire pi√π dataset
5. **Versioning dati** per storico modifiche
6. **API pagination** per dataset molto grandi
7. **GraphQL endpoint** (alternativa a REST, opzionale)

---

## 12. Domande per Chiarimenti

Prima di procedere all'implementazione, confermare:

1. ‚úÖ **Autenticazione admin**: Username/password + JWT √® sufficiente, o serve integrazione LDAP/OAuth?

2. ‚úÖ **Rotazione API keys**: Rotazione manuale periodica √® accettabile, o serve automazione?

3. ‚úÖ **Gestione file GeoJSON**: Anche GeoJSON (`Ambiti geolinguistici newline.json`) deve essere generato da CSV, o pu√≤ rimanere statico?

4. ‚úÖ **Validazione CSV**: Quali validazioni specifiche sul CSV? (es. campi obbligatori, formati, range valori)

5. ‚úÖ **Notifiche upload**: Email/Slack notification quando upload completato?

6. ‚úÖ **Interfaccia admin**: Serve UI web per upload, o CLI/API √® sufficiente?

7. ‚úÖ **Backup/Rollback**: Backup automatico giornaliero √® sufficiente? Serve rollback UI?

8. ‚úÖ **Performance**: Quanti utenti concorrenti previsti? (per dimensionare rate limits)

9. ‚úÖ **Compliance**: Requisiti GDPR/privacy specifici oltre quelli standard?

10. ‚úÖ **Budget deployment**: Hosting self-managed o cloud managed (AWS/Azure)?

---

**Documento pronto per revisione e approvazione**  
**Prossimo step**: Feedback su requisiti ‚Üí Implementazione Fase 1
